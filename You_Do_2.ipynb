{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "You_Do_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHIMpl/yHCXUhgsnYUbVdN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsemihkoca/YouDo-ds-bc/blob/main/You_Do_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz9nmthnDBSg",
        "outputId": "87e672c1-4b47-4370-9704-66641df5e866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://files.grouplens.org/datasets/movielens/ml-100k/u.data', delimiter=r'\\t',names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
        "r = df.pivot(index='user_id', columns='item_id', values='rating').values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0 Train-Test Split"
      ],
      "metadata": {
        "id": "_TiHXmFFPFKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "irow, jcol = np.where(~np.isnan(r))\n",
        "n=int(len(irow)*0.2)\n",
        "\n",
        "idx = np.random.choice(np.arange(len(irow)), n, replace=False)"
      ],
      "metadata": {
        "id": "uVe4m4d2KyEK"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_irow = irow[idx]\n",
        "test_jcol = jcol[idx]\n",
        "jcol[idx].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuH25LxYK333",
        "outputId": "765f81d4-0053-4d88-deb7-3f523443a52b"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000,)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r_original=r.copy()\n",
        "r_copy = r.copy()\n",
        "l=0\n",
        "for i in range(n):\n",
        "  r_copy[test_irow[i]][test_jcol[i]] = np.nan\n",
        "  l+=1"
      ],
      "metadata": {
        "id": "XjBa_cDCK5uC"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(r_copy).sum()-np.isnan(r_original).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNfAqZnTK6ND",
        "outputId": "a8dd6fec-e1df-426b-e3dd-c9bffe20f5b4"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users,items=r_copy.shape"
      ],
      "metadata": {
        "id": "RXVXf4MkLA0D"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loss(r,b_users,b_items,irow, jcol):\n",
        "  Train_Loss=0\n",
        "  for u, j in zip(irow, jcol):\n",
        "    Train_Loss+=np.nansum((r[u][j] - (b_users[u] + b_items[j]))**2)*0.5\n",
        "  return Train_Loss\n",
        "\n",
        "def test_loss(r,b_users,b_items,test_irow, test_jcol):\n",
        "  Test_Loss=0\n",
        "  for u, j in zip(test_irow, test_jcol):\n",
        "    Test_Loss+=np.nansum((r[u][j] -  (b_users[u] + b_items[j]))**2)*0.5\n",
        "  return Test_Loss"
      ],
      "metadata": {
        "id": "Vs3-4BENNW-w"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 Optimization "
      ],
      "metadata": {
        "id": "b3lIoCFirybb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_users=np.random.rand(users,1)\n",
        "grad_b_users=np.random.rand(users,1)\n",
        "\n",
        "b_items=np.random.rand(items,1)\n",
        "grad_b_items=np.random.rand(items,1)\n",
        "\n",
        "alpha=(3)*(10**-2)\n",
        "max_iter=100\n",
        "r_pred=np.full((r_copy.shape),np.nan)\n",
        "\n",
        "for iter in range(max_iter):\n",
        "  for i in range(users):\n",
        "    for j in range(items):\n",
        "      if np.isnan(r_copy[i][j]): # zaten bilmediğim değeri tahmin edemem NANları nasıl tahmin ederim\n",
        "            continue\n",
        "      else:\n",
        "        try:\n",
        "\n",
        "          r_pred[i][j]= b_users[i] + b_items[j]\n",
        "\n",
        "          grad_b_users[i]=-(r_copy[i][j]-r_pred[i][j])\n",
        "          grad_b_items[j]=-(r_copy[i][j]-r_pred[i][j])\n",
        "\n",
        "          b_users_prev=b_users.copy()\n",
        "\n",
        "          b_users[i] = b_users[i]- alpha * grad_b_users[i]\n",
        "          b_items[j] = b_items[j]- alpha * grad_b_items[j]\n",
        "\n",
        "\n",
        "        except ValueError:\n",
        "          print(\"ValueError encountered\")\n",
        "  Train_Loss= train_loss(r_copy,b_users,b_items,irow, jcol)\n",
        "  Test_Loss= test_loss(r_original,b_users,b_items,test_irow, test_jcol)\n",
        "\n",
        "  if(iter%1 == 0):\n",
        "    print(f\"iteration: ({iter}) , gradient: {np.linalg.norm(grad_b_users)+np.linalg.norm(grad_b_items):.3f}, Train_Loss: {Train_Loss:.3f}, Test_Loss: {Test_Loss:.3f}\")\n",
        "            \n",
        "  # if np.linalg.norm(b_users-b_users_prev) < 0.000001:\n",
        "  #   print(f\"I do early stoping at iteration {iter}\")\n",
        "  #   break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SOa98U_Mn8G",
        "outputId": "82258386-a6ca-439d-8aaa-526063cf0866"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: (0) , gradient: 96.649, Train_Loss: 82508.109, Test_Loss: 21168.831\n",
            "iteration: (1) , gradient: 81.200, Train_Loss: 38516.981, Test_Loss: 10256.688\n",
            "iteration: (2) , gradient: 75.962, Train_Loss: 36030.552, Test_Loss: 9643.081\n",
            "iteration: (3) , gradient: 73.306, Train_Loss: 35253.462, Test_Loss: 9451.695\n",
            "iteration: (4) , gradient: 71.648, Train_Loss: 34901.887, Test_Loss: 9366.439\n",
            "iteration: (5) , gradient: 70.476, Train_Loss: 34708.773, Test_Loss: 9321.199\n",
            "iteration: (6) , gradient: 69.581, Train_Loss: 34588.120, Test_Loss: 9294.342\n",
            "iteration: (7) , gradient: 68.865, Train_Loss: 34505.539, Test_Loss: 9277.127\n",
            "iteration: (8) , gradient: 68.272, Train_Loss: 34445.172, Test_Loss: 9265.491\n",
            "iteration: (9) , gradient: 67.771, Train_Loss: 34398.883, Test_Loss: 9257.329\n",
            "iteration: (10) , gradient: 67.339, Train_Loss: 34362.121, Test_Loss: 9251.454\n",
            "iteration: (11) , gradient: 66.963, Train_Loss: 34332.149, Test_Loss: 9247.151\n",
            "iteration: (12) , gradient: 66.632, Train_Loss: 34307.219, Test_Loss: 9243.963\n",
            "iteration: (13) , gradient: 66.339, Train_Loss: 34286.157, Test_Loss: 9241.583\n",
            "iteration: (14) , gradient: 66.077, Train_Loss: 34268.136, Test_Loss: 9239.803\n",
            "iteration: (15) , gradient: 65.841, Train_Loss: 34252.559, Test_Loss: 9238.475\n",
            "iteration: (16) , gradient: 65.629, Train_Loss: 34238.977, Test_Loss: 9237.490\n",
            "iteration: (17) , gradient: 65.436, Train_Loss: 34227.047, Test_Loss: 9236.770\n",
            "iteration: (18) , gradient: 65.261, Train_Loss: 34216.500, Test_Loss: 9236.256\n",
            "iteration: (19) , gradient: 65.101, Train_Loss: 34207.124, Test_Loss: 9235.903\n",
            "iteration: (20) , gradient: 64.955, Train_Loss: 34198.747, Test_Loss: 9235.678\n",
            "iteration: (21) , gradient: 64.820, Train_Loss: 34191.230, Test_Loss: 9235.555\n",
            "iteration: (22) , gradient: 64.697, Train_Loss: 34184.458, Test_Loss: 9235.512\n",
            "iteration: (23) , gradient: 64.583, Train_Loss: 34178.335, Test_Loss: 9235.534\n",
            "iteration: (24) , gradient: 64.478, Train_Loss: 34172.781, Test_Loss: 9235.609\n",
            "iteration: (25) , gradient: 64.381, Train_Loss: 34167.728, Test_Loss: 9235.727\n",
            "iteration: (26) , gradient: 64.290, Train_Loss: 34163.121, Test_Loss: 9235.879\n",
            "iteration: (27) , gradient: 64.207, Train_Loss: 34158.909, Test_Loss: 9236.060\n",
            "iteration: (28) , gradient: 64.129, Train_Loss: 34155.050, Test_Loss: 9236.264\n",
            "iteration: (29) , gradient: 64.057, Train_Loss: 34151.508, Test_Loss: 9236.486\n",
            "iteration: (30) , gradient: 63.990, Train_Loss: 34148.252, Test_Loss: 9236.724\n",
            "iteration: (31) , gradient: 63.928, Train_Loss: 34145.254, Test_Loss: 9236.975\n",
            "iteration: (32) , gradient: 63.869, Train_Loss: 34142.489, Test_Loss: 9237.236\n",
            "iteration: (33) , gradient: 63.815, Train_Loss: 34139.936, Test_Loss: 9237.505\n",
            "iteration: (34) , gradient: 63.764, Train_Loss: 34137.576, Test_Loss: 9237.781\n",
            "iteration: (35) , gradient: 63.717, Train_Loss: 34135.393, Test_Loss: 9238.062\n",
            "iteration: (36) , gradient: 63.673, Train_Loss: 34133.371, Test_Loss: 9238.347\n",
            "iteration: (37) , gradient: 63.631, Train_Loss: 34131.497, Test_Loss: 9238.635\n",
            "iteration: (38) , gradient: 63.592, Train_Loss: 34129.759, Test_Loss: 9238.925\n",
            "iteration: (39) , gradient: 63.556, Train_Loss: 34128.147, Test_Loss: 9239.217\n",
            "iteration: (40) , gradient: 63.522, Train_Loss: 34126.649, Test_Loss: 9239.509\n",
            "iteration: (41) , gradient: 63.490, Train_Loss: 34125.259, Test_Loss: 9239.801\n",
            "iteration: (42) , gradient: 63.460, Train_Loss: 34123.966, Test_Loss: 9240.093\n",
            "iteration: (43) , gradient: 63.432, Train_Loss: 34122.765, Test_Loss: 9240.383\n",
            "iteration: (44) , gradient: 63.406, Train_Loss: 34121.649, Test_Loss: 9240.673\n",
            "iteration: (45) , gradient: 63.381, Train_Loss: 34120.610, Test_Loss: 9240.961\n",
            "iteration: (46) , gradient: 63.358, Train_Loss: 34119.645, Test_Loss: 9241.246\n",
            "iteration: (47) , gradient: 63.336, Train_Loss: 34118.747, Test_Loss: 9241.530\n",
            "iteration: (48) , gradient: 63.316, Train_Loss: 34117.911, Test_Loss: 9241.811\n",
            "iteration: (49) , gradient: 63.297, Train_Loss: 34117.135, Test_Loss: 9242.089\n",
            "iteration: (50) , gradient: 63.279, Train_Loss: 34116.412, Test_Loss: 9242.365\n",
            "iteration: (51) , gradient: 63.262, Train_Loss: 34115.741, Test_Loss: 9242.637\n",
            "iteration: (52) , gradient: 63.246, Train_Loss: 34115.117, Test_Loss: 9242.906\n",
            "iteration: (53) , gradient: 63.231, Train_Loss: 34114.537, Test_Loss: 9243.171\n",
            "iteration: (54) , gradient: 63.217, Train_Loss: 34113.998, Test_Loss: 9243.433\n",
            "iteration: (55) , gradient: 63.203, Train_Loss: 34113.498, Test_Loss: 9243.692\n",
            "iteration: (56) , gradient: 63.191, Train_Loss: 34113.033, Test_Loss: 9243.946\n",
            "iteration: (57) , gradient: 63.179, Train_Loss: 34112.603, Test_Loss: 9244.197\n",
            "iteration: (58) , gradient: 63.168, Train_Loss: 34112.203, Test_Loss: 9244.444\n",
            "iteration: (59) , gradient: 63.158, Train_Loss: 34111.833, Test_Loss: 9244.687\n",
            "iteration: (60) , gradient: 63.148, Train_Loss: 34111.491, Test_Loss: 9244.926\n",
            "iteration: (61) , gradient: 63.139, Train_Loss: 34111.174, Test_Loss: 9245.161\n",
            "iteration: (62) , gradient: 63.130, Train_Loss: 34110.881, Test_Loss: 9245.392\n",
            "iteration: (63) , gradient: 63.122, Train_Loss: 34110.610, Test_Loss: 9245.619\n",
            "iteration: (64) , gradient: 63.114, Train_Loss: 34110.361, Test_Loss: 9245.841\n",
            "iteration: (65) , gradient: 63.107, Train_Loss: 34110.131, Test_Loss: 9246.060\n",
            "iteration: (66) , gradient: 63.100, Train_Loss: 34109.920, Test_Loss: 9246.275\n",
            "iteration: (67) , gradient: 63.093, Train_Loss: 34109.725, Test_Loss: 9246.485\n",
            "iteration: (68) , gradient: 63.087, Train_Loss: 34109.547, Test_Loss: 9246.692\n",
            "iteration: (69) , gradient: 63.081, Train_Loss: 34109.384, Test_Loss: 9246.894\n",
            "iteration: (70) , gradient: 63.076, Train_Loss: 34109.235, Test_Loss: 9247.093\n",
            "iteration: (71) , gradient: 63.071, Train_Loss: 34109.099, Test_Loss: 9247.287\n",
            "iteration: (72) , gradient: 63.066, Train_Loss: 34108.975, Test_Loss: 9247.477\n",
            "iteration: (73) , gradient: 63.062, Train_Loss: 34108.862, Test_Loss: 9247.664\n",
            "iteration: (74) , gradient: 63.057, Train_Loss: 34108.761, Test_Loss: 9247.846\n",
            "iteration: (75) , gradient: 63.053, Train_Loss: 34108.669, Test_Loss: 9248.025\n",
            "iteration: (76) , gradient: 63.049, Train_Loss: 34108.587, Test_Loss: 9248.200\n",
            "iteration: (77) , gradient: 63.046, Train_Loss: 34108.514, Test_Loss: 9248.371\n",
            "iteration: (78) , gradient: 63.042, Train_Loss: 34108.448, Test_Loss: 9248.538\n",
            "iteration: (79) , gradient: 63.039, Train_Loss: 34108.391, Test_Loss: 9248.702\n",
            "iteration: (80) , gradient: 63.036, Train_Loss: 34108.340, Test_Loss: 9248.862\n",
            "iteration: (81) , gradient: 63.033, Train_Loss: 34108.296, Test_Loss: 9249.018\n",
            "iteration: (82) , gradient: 63.031, Train_Loss: 34108.258, Test_Loss: 9249.171\n",
            "iteration: (83) , gradient: 63.028, Train_Loss: 34108.226, Test_Loss: 9249.321\n",
            "iteration: (84) , gradient: 63.026, Train_Loss: 34108.199, Test_Loss: 9249.467\n",
            "iteration: (85) , gradient: 63.023, Train_Loss: 34108.177, Test_Loss: 9249.609\n",
            "iteration: (86) , gradient: 63.021, Train_Loss: 34108.160, Test_Loss: 9249.749\n",
            "iteration: (87) , gradient: 63.019, Train_Loss: 34108.147, Test_Loss: 9249.885\n",
            "iteration: (88) , gradient: 63.017, Train_Loss: 34108.138, Test_Loss: 9250.017\n",
            "iteration: (89) , gradient: 63.016, Train_Loss: 34108.133, Test_Loss: 9250.147\n",
            "iteration: (90) , gradient: 63.014, Train_Loss: 34108.131, Test_Loss: 9250.274\n",
            "iteration: (91) , gradient: 63.012, Train_Loss: 34108.132, Test_Loss: 9250.397\n",
            "iteration: (92) , gradient: 63.011, Train_Loss: 34108.136, Test_Loss: 9250.518\n",
            "iteration: (93) , gradient: 63.009, Train_Loss: 34108.143, Test_Loss: 9250.635\n",
            "iteration: (94) , gradient: 63.008, Train_Loss: 34108.152, Test_Loss: 9250.750\n",
            "iteration: (95) , gradient: 63.007, Train_Loss: 34108.164, Test_Loss: 9250.862\n",
            "iteration: (96) , gradient: 63.005, Train_Loss: 34108.177, Test_Loss: 9250.971\n",
            "iteration: (97) , gradient: 63.004, Train_Loss: 34108.193, Test_Loss: 9251.078\n",
            "iteration: (98) , gradient: 63.003, Train_Loss: 34108.210, Test_Loss: 9251.181\n",
            "iteration: (99) , gradient: 63.002, Train_Loss: 34108.229, Test_Loss: 9251.283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "\n",
        "a=pd.DataFrame(r_pred[:5,:10]).fillna('-', inplace=False)\n",
        "b=pd.DataFrame(r_copy[:5,:10]).fillna('-', inplace=False)\n",
        "print(\"                  r_pred vs r_copy\\n\")\n",
        "pd.concat([a,b],axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "4Jqqeo5GPvQX",
        "outputId": "a29885f7-68d4-41a8-d7db-e6fd75f4852d"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  r_pred vs r_copy\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0    1    2    3    4    5    6    7    8    9\n",
              "0    - 3.15 2.77 3.40 3.09 3.78 3.75 3.85 3.70 3.65\n",
              "1 3.88    -    -    -    -    -    -    -    - 3.89\n",
              "2    -    -    -    -    -    -    -    -    -    -\n",
              "3    -    -    -    -    -    -    -    -    -    -\n",
              "4 3.30    -    -    -    -    -    -    -    -    -\n",
              "0    - 3.00 4.00 3.00 3.00 5.00 4.00 1.00 5.00 3.00\n",
              "1 4.00    -    -    -    -    -    -    -    - 2.00\n",
              "2    -    -    -    -    -    -    -    -    -    -\n",
              "3    -    -    -    -    -    -    -    -    -    -\n",
              "4 4.00    -    -    -    -    -    -    -    -    -"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ead71eef-256f-4260-8829-f5c96d62b21f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.77</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.09</td>\n",
              "      <td>3.78</td>\n",
              "      <td>3.75</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.70</td>\n",
              "      <td>3.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.88</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>3.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.30</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-</td>\n",
              "      <td>3.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.00</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.00</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ead71eef-256f-4260-8829-f5c96d62b21f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ead71eef-256f-4260-8829-f5c96d62b21f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ead71eef-256f-4260-8829-f5c96d62b21f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 Regularized Model"
      ],
      "metadata": {
        "id": "x607gkzbsUxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loss_regularized(r,b_users,b_items,irow, jcol,lamb):\n",
        "  Train_Loss=0\n",
        "  for u, j in zip(irow, jcol):\n",
        "    Train_Loss+=np.nansum((r[u][j] - (b_users[u] + b_items[j]))**2)*0.5 + (b_users[u]**2 + b_items[j]**2)*lamb*0.5\n",
        "  return float(Train_Loss[0])\n",
        "\n",
        "def test_loss_regularized(r,b_users,b_items,test_irow, test_jcol,lamb):\n",
        "  Test_Loss=0\n",
        "  for u, j in zip(test_irow, test_jcol):\n",
        "    Test_Loss+=np.nansum((r[u][j] -  (b_users[u] + b_items[j]))**2)*0.5 + (b_users[u]**2 + b_items[j]**2)*lamb*0.5\n",
        "  return float(Test_Loss[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "e1iU1W7TJLL7"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Regularized_SGD(lamb):\n",
        "\n",
        "  b_users=np.random.rand(users,1)\n",
        "  grad_b_users=np.random.rand(users,1)\n",
        "\n",
        "  b_items=np.random.rand(items,1)\n",
        "  grad_b_items=np.random.rand(items,1)\n",
        "\n",
        "  r_pred=np.full((r_copy.shape),np.nan)\n",
        "\n",
        "  alpha=(3)*(10**-2)\n",
        "  Test_Loss_list=[]\n",
        "  Info=False\n",
        "  max_iter=3\n",
        "\n",
        "  for iter in range(max_iter):\n",
        "    for i in range(users):\n",
        "      for j in range(items):\n",
        "        if np.isnan(r_copy[i][j]): # zaten bilmediğim değeri tahmin edemem NANları nasıl tahmin ederim\n",
        "              continue\n",
        "        else:\n",
        "          try:\n",
        "\n",
        "            r_pred[i][j]= b_users[i] + b_items[j]\n",
        "\n",
        "            grad_b_users[i]=-(r_copy[i][j]-r_pred[i][j]) + lamb * b_users[i]\n",
        "            grad_b_items[j]=-(r_copy[i][j]-r_pred[i][j]) + lamb * b_items[j]\n",
        "\n",
        "            b_users_prev=b_users.copy()\n",
        "\n",
        "            b_users[i] = b_users[i]- alpha * grad_b_users[i]\n",
        "            b_items[j] = b_items[j]- alpha * grad_b_items[j]\n",
        "\n",
        "\n",
        "          except ValueError:\n",
        "            print(\"ValueError encountered\")\n",
        "    Train_Loss= train_loss_regularized(r_copy,b_users,b_items,irow, jcol,lamb)\n",
        "    Test_Loss= test_loss_regularized(r_original,b_users,b_items,test_irow, test_jcol,lamb)\n",
        "    Test_Loss_list.append(Test_Loss)\n",
        "    if Info:\n",
        "      if(iter%1 == 0):\n",
        "        print(f\"iteration: ({iter}) , gradient: {(np.linalg.norm(grad_b_users)+np.linalg.norm(grad_b_items)):.3f}, Reg_Train_Loss: {Train_Loss:.3f}, Reg_Test_Loss: {Test_Loss:.3f}\")\n",
        "        \n",
        "  return -min(Test_Loss_list)\n",
        "\n",
        "max_iter=3\n",
        "lamb=0.001\n",
        "\n",
        "Regularized_SGD(lamb)\n",
        "\n",
        "\n",
        "    # return r_pred,b_users, b_items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xNWcIhUsa9S",
        "outputId": "81d761ce-9c74-454b-d8b4-74286d493644"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9737.001887101575"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 HyperParam Opt"
      ],
      "metadata": {
        "id": "Hvz9OicCSBIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuATXemtSPdF",
        "outputId": "b61c6791-3022-40ea-cfad-8e746881dbb1"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Regularized_SGD(max_iter,lamb,Info=False)\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'lamb': (0.00005,0.1)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=Regularized_SGD,\n",
        "    pbounds=pbounds,\n",
        "    random_state=1,\n",
        ")"
      ],
      "metadata": {
        "id": "9bldm-jnSXS0"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.maximize(\n",
        "    init_points=0.0001,\n",
        "    n_iter=10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecAQHSanWfUr",
        "outputId": "0800454a-eb0f-4bdd-bfe0-8d15cbe04136"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   lamb    |\n",
            "-------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-1.228e+0\u001b[0m | \u001b[0m 0.04173 \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m-1.581e+0\u001b[0m | \u001b[0m 0.1     \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-1.228e+0\u001b[0m | \u001b[0m 0.04174 \u001b[0m |\n",
            "| \u001b[95m 4       \u001b[0m | \u001b[95m-1.196e+0\u001b[0m | \u001b[95m 0.03687 \u001b[0m |\n",
            "| \u001b[95m 5       \u001b[0m | \u001b[95m-1.144e+0\u001b[0m | \u001b[95m 0.02818 \u001b[0m |\n",
            "| \u001b[95m 6       \u001b[0m | \u001b[95m-1.108e+0\u001b[0m | \u001b[95m 0.02262 \u001b[0m |\n",
            "| \u001b[95m 7       \u001b[0m | \u001b[95m-9.635e+0\u001b[0m | \u001b[95m 5e-05   \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-9.635e+0\u001b[0m | \u001b[0m 5e-05   \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-9.635e+0\u001b[0m | \u001b[0m 5e-05   \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-9.635e+0\u001b[0m | \u001b[0m 5e-05   \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-9.635e+0\u001b[0m | \u001b[0m 5e-05   \u001b[0m |\n",
            "=====================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best lambda value: {optimizer.max['params']['lamb']:.5f}\\nRegular_Test_Loss: {-optimizer.max['target']:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo_3qfqJYKbu",
        "outputId": "d2ffbaf7-a4a5-4ec2-b75d-83bf9965b8bc"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best lambda value: 0.00005\n",
            "Regular_Test_Loss: 9635.30415\n"
          ]
        }
      ]
    }
  ]
}